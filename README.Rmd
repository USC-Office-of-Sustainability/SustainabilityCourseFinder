---
title: "USC Sustainability Course Finder"
output: 
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


<!-- # USC Sustainability Course Finder -->

```{r, warning=FALSE,message=FALSE,echo=FALSE}
# load all packages
library(tidyverse)
library(knitr)
```

## Introduction

Peter Wu at Carnegie Mellon wrote the initial code that inspired this project, and his original R package can be found on [Github](https://github.com/pwu97/SDGmapR){target='_blank'}. 
At USC, Brian Tinsley, Alison Chen and Dr. Julie Hopper in the Office of Sustainability switched to using the new [text2sdg](https://www.text2sdg.io/){target='_blank'} package to raise sustainability awareness in higher education by mapping USC course descriptions to the [United Nations Sustainability Development Goals](https://sdgs.un.org/goals){target='_blank'}.

Check out the [Sustainability Course Finder](https://usc-sustainability.shinyapps.io/Sustainability-Course-Finder/){target="_blank"} to see the the product of our work! Also find an article about our web app [here](https://news.usc.edu/207748/new-usc-sustainability-course-finder/){target="_blank"}!


## Installation

Prerequisites: R and RStudio

If you wish to install this package on your computer, clone this repository by following [these instructions](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository){target="_blank"}.
Once downloaded, you can open, view, and edit all files in this repository.  

Open RStudio and click on the button in the top right to open the project file `SustainabilityCourseFinder.Rproj`. This will automatically set the working directory as the project directory. For more information about Projects [here](https://support.posit.co/hc/en-us/articles/200526207-Using-RStudio-Projects#:~:text=Opening%20Projects,Rproj){target="_blank"}.

```{r, eval=FALSE}
# check current directory
getwd()
```

For those who are new to R, we often install and load external packages at the top of our R scripts like this:

```{r, eval=FALSE}
# install the tidyverse package
install.packages("tidyverse")
# load the package into our library so we can access its functions
library(tidyverse)

```

The [tidyverse package](https://github.com/tidyverse/tidyverse){target="_blank"} is an incredibly powerful R package which helps transform and present data; it has been used extensively in this project.  


## Keyword List

The way in which we map course descriptions to the SDGs is through keyword lists containing words relevant to each SDG. The table below lists publicly available SDG keywords that have been published online.  

Some of the lists have weights associated with every keyword based on their relevance to the SDG, while some do not. Also note that some of these keyword lists do not have keywords for SDG 17. In prior versions of this project, we attempted to mimic CMU's method and use various Python and R packages to assign weights but in our current version we do not use weights.

Source | Dataset | CSV
---|---|---
USC Keywords (Work in Progress) | `usc_keywords` | <a href="https://github.com/USC-Office-of-Sustainability/SustainabilityCourseFinder/blob/main/shiny_app/usc_keywords.csv" target="_blank">Link</a>
<a href="https://data.mendeley.com/datasets/87txkw7khs/1" target="_blank">Core Elsevier (Work in Progress)</a> | `elsevier_keywords` | <a href="https://github.com/pwu97/SDGmapR/blob/main/datasets/elsevier_keywords_cleaned.csv" target="_blank">Link</a>
<a href="https://data.mendeley.com/datasets/9sxdykm8s4/2" target="_blank">Improved Elsevier Top 100</a> | `elsevier100_keywords` | <a href="https://github.com/pwu97/SDGmapR/blob/main/datasets/elsevier100_keywords_cleaned.csv" target="_blank">Link</a>
<a href="https://ap-unsdsn.org/regional-initiatives/universities-sdgs/" target="_blank">SDSN</a> | `sdsn_keywords` | <a href="https://github.com/pwu97/SDGmapR/blob/main/datasets/sdsn_keywords_cleaned.csv" target="_blank">Link</a>
<a href="https://www.cmu.edu/leadership/the-provost/provost-priorities/sustainability-initiative/sdg-definitions.html" target="_blank">CMU Top 250 Words</a> | `cmu250_keywords` | <a href="https://github.com/pwu97/SDGmapR/blob/main/datasets/cmu250_keywords_cleaned.csv" target="_blank">Link</a>
<a href="https://www.sdgmapping.auckland.ac.nz/" target="_blank">University of Auckland (Work in Progress)</a> | `auckland_keywords` |
<a href="https://data.utoronto.ca/sustainable-development-goals-sdg-report/sdg-report-appendix/" target="_blank">University of Toronto (Work in Progress)</a> | `toronto_keywords` | 

Additional keywords can be accessed via [text2sdg](https://www.text2sdg.io/reference/detect_sdg_systems.html){target="_blank"}.

The first few rows of the USC keyword table, which has over 4250 keywords, are shown below.

```{r, echo=F}
keywords = read.csv("shiny_app/usc_keywords.csv")
kw = keywords %>% select(goal, keyword, color) %>% (head)
kable(kw)
```

The USC keyword list has been modified many times with the help of the Presidential Working Group (PWG) and is continually being improved to increase accuracy. 

In the 02_R directory's file `01_cleaning_keywords.R`, notice that the keywords are converted to lowercase, punctuation is removed, and that duplicates are removed. Removing duplicates is very important for ensuring some courses do not get mapped twice. Furthermore, the pound symbol causes problems when using [text2sdg](https://www.text2sdg.io/){target='_blank'}'s `detect_any()` so keywords with `#` are removed.

```{r, code = readLines("02_R/01_cleaning_keywords.R"), eval=F}
```

## Cleaning Course Data

While we do not expect another school's data to be of the same format as the raw files at USC, we are still including some details on how we cleaned the files in hopes that it may address some common problems others might have with their data.  

<!-- add link to a file? -->
Course data was retrieved from the USCâ€™s Office of Academic Records and Registrar, and the raw data files, as well as the R scripts to clean them, can be found in the `01_cleaning_raw_data/00_raw_usc_data` folder [here](https://github.com/USC-Office-of-Sustainability/SustainabilityCourseFinder/tree/main/01_cleaning_raw_data/00_raw_usc_data){target='_blank'}.
The raw data files had lots of problems with spacing and column names, and we addressed these issues in `01_combine_SOC.R`.  

The main problem was that one course's description was sometimes spread over two cells instead of one row per course. This occurred in multiple columns. For instance, the data looked like this:

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
library(readxl)
d <- read_excel("01_cleaning_raw_data/00_raw_usc_data/SOC_files/20232_SOC.xlsx", col_types = "text")
# remove . in colname
names(d) <- gsub("\\.", "", names(d))
# rename total_enr
total_enr_i <- grep("TOTAL_ENR", names(d))
for (i in 1:length(total_enr_i)) {
  names(d)[total_enr_i][i] <- paste0("TOTAL_ENR", i)
}
# rename PUBLISH?
names(d)[grep("PUBLISH", names(d))] <- "PUBLISH"
kable(d[19:24,], caption = "An example of raw data for course IDSN-585 spread over multiple rows.")
```

The key to combining multiple rows as one row was tidyr's fill function shown below.
```{r eval=FALSE}
# fill empty columns based on above column
df2 <- df %>%
  tidyr::fill(SECTION, COURSE_CODE, .direction = "down")
```
By filling in the SECTION and COURSE_CODE columns with the previous row, the dataframe looks like this:
```{r echo=FALSE}
library(dplyr)
d1 <- d[19:24,] %>%
  tidyr::fill(SECTION, COURSE_CODE, .direction = "down")
kable(d1, caption = "SECTION and COURSE_CODE columns are filled in based on the first row.")
```
Now, we simply group by SECTION and COURSE_CODE and combine all the text in each column to end up with one row for one course:
```{r echo=FALSE, message=FALSE}
d2 <- d1 %>%
  group_by(SECTION, COURSE_CODE) %>%
    summarize(SCHOOL = paste(SCHOOL[!is.na(SCHOOL)], collapse = " "),
              SESSION = first(SESSION),
              MIN_UNITS = first(MIN_UNITS),
              MAX_UNITS = first(MAX_UNITS),
              COURSE_TITLE = paste(COURSE_TITLE[!is.na(COURSE_TITLE)], collapse = " "),
              MODE = first(MODE),
              Link = first(Link),
              PUBLISH = first(PUBLISH),
              START_TIME = paste(START_TIME[!is.na(START_TIME)], collapse = " "),
              END_TIME = paste(END_TIME[!is.na(END_TIME)], collapse = " "),
              DAYS = paste(DAYS[!is.na(DAYS)], collapse = " "),
              TOTAL_ENR = first(TOTAL_ENR1),
              MODALITY = first(MODALITY),
              INSTRUCTOR_NAME = paste(INSTRUCTOR_NAME[!is.na(INSTRUCTOR_NAME)], collapse = ";"),
              ASSIGNED_ROOM = paste(ASSIGNED_ROOM[!is.na(ASSIGNED_ROOM)], collapse = " "),
              TOTAL_ENR1 = first(TOTAL_ENR2),
              COURSE_DESCRIPTION = paste(COURSE_DESCRIPTION[!is.na(COURSE_DESCRIPTION)], collapse = " "))
kable(d2, caption = "Final row for course IDSN-585.")
```

The process is the same for both CSV and Excel files. The only difference is that CSV files have an extra DEPTOWNERNAME column. While cleaning the SOC files, we also added an "origin" column which indicates which year and term the data is from.

Once all the files have been cleaned and saved in a new folder clean_data, we can combine them all into one dataframe with the following code:

```{r, eval=FALSE}
ff <- list.files("01_cleaning_raw_data/00_raw_usc_data/clean_data", 
                 pattern = "csv", full.names = TRUE)
# read data
tmp <- lapply(ff, read.csv)

# combine
combined_data <- data.table::rbindlist(tmp, fill = TRUE)

write.csv(combined_data,
          "combined_data.csv",
          row.names = FALSE)
```

In the next R file, `02_formatting.R`, we read in the combined clean CSV and reformat it. In this file, we change column names, count the number of students and sections for each section, cut out courses listed purely for enrollment credit, and we create the "semester," "all_semesters," and "course_level" columns. To see this code please see the R script [here](https://github.com/USC-Office-of-Sustainability/SustainabilityCourseFinder/tree/main/01_cleaning_raw_data/00_raw_usc_data/02_formatting.R){target='_blank'}. In this script, some of the cleaning is done in one function, `clean_data`, and some cleaning processes are done with helper functions like `get_semester` and `get_course_level`.

One important piece of this file is excluding certain courses. For example, courses with titles containing "Directed Research" and Individual Instruction", courses with exact titles "Advanced Research Experience" and Board Development", courses with descriptions containing "Directed undergraduate research" and "Directed graduate research", and courses with course IDs ending in 490, 790, and 594 are all removed. You can add additional rules to the clean_data function:

```{r, eval=F}
# a snippet of code from 02_cleaning_2020-2023.R
titles_containing = c("Directed Research",
                        "Individual Instruction")
titles_matching = c("Advanced Research Experience",
                      "Board Development")
descriptions_containing = c("Directed undergraduate research",
                              "Directed graduate research")
data_clean <- raw_data %>%
    filter(!grepl(paste(titles_containing, collapse = "|"), COURSE_TITLE) & 
             !COURSE_TITLE %in% titles_matching &
             !grepl(paste(descriptions_containing, collapse = "|"), COURSE_DESCRIPTION) &
             !grepl("-[47]90|-594", COURSE_CODE))
```

Note that when you make an update to data like such, you must go and rerun the next R scripts with the new data to update all of the data frames for the shiny app. 
Once we create the cleaned `usc_courses.csv`, we duplicate it and move it up one directory so that we can run more code on it.

In the above directory, `01_cleaning_raw_data`, there is an R script that shows you how to add a course to the dataframe `01_adding_course.R`. It is important that you include ALL COLUMNS when adding new entries -- otherwise the data will get messy. 


## Cleaning Course Descriptions

Once we have the cleaned dataframe with correct column names, we now clean the course descriptions in `01_cleaning_raw_data/03_cleaning_course_descriptions.R` to increase the accuracy of the mapping we will perform to the keyword list.

This file corrects context-dependency issues that lead to inaccurate mappings of courses to SDGs. For example, courses with the phrases "business environment" or "learning environment" should not be mapped to the word "environment" and its related SDGs. 

First some typos in the course descriptions are corrected using `stri_replace_all_regex` from the [stringi package](https://stringi.gagolewski.com/){target='_blank'}.

Next we want to create a new column "clean_course_desc" which holds the course description of the course without punctuation  except apostrophes and corrected context dependencies. 

```{r eval=FALSE}
usc_courses$clean_course_desc <- 
  apply_context_dependency(remove_punctuation(usc_courses$course_desc))
```

The `remove_punctuation` function simply replaces all punctuation in the text with a space using gsub. Learn more about regular expressions in R by typing `?base::regex` into the console.

```{r eval = FALSE}
remove_punctuation <- function(tt) {
  gsub("[^[:alnum:][:space:]']", " ", tt)
}
```

The `apply_context_dependency` function uses `stri_replace_all_regex` to replace advertising ecosystem with advertising domain in all course descriptions. There is a file called `context_dependencies.csv` which lists all the replacements to be made as two columns: before and after. You can use regex capture groups for more generic matches. Warning: the more context dependencies in the csv file, the slower this function will run. 

Once we have the output from this cleaning, `usc_courses_cleaned.csv`, we duplicate it and move it to the `data` directory where we will be working from now on. Now, all of our R scripts will be from the `02_R` directory and our data will be in this data directory. 


## Mapping Course Descriptions with text2sdg

Our previous strategy to map course descriptions took over 6 hours to run. We now use [text2sdg](https://www.text2sdg.io/){target='_blank'}'s `detect_any` function to map course descriptions in less than 5 minutes.

Now, we are ready to map the clean course descriptions void of punctuation errors and major context dependencies to our keyword list and the 17 SDGs. 
In the `02_R` directory, find the code to map course descriptions in `02_using_text2sdg.R`.  
First we need to create a system to use the USC keywords. The system needs to have 3 columns: system name, SDG, and query.

```{r eval=FALSE}
# create system for text2sdg
usc_pwg_system <- usc_pwg_keywords %>%
  mutate(system = "usc_pwg",
         query = paste0('"', keyword, '"')) %>%
  rename(sdg = goal) %>%
  select(system, sdg, query)
```

Make sure the keywords and the text (course description) have no punctuation and are lowercase, so that detect_any can find the keywords in the text.

Next we run detect_any using our keyword system. This function will only count a keyword once. The output from this function is a dataframe with columns: document, sdg, system, query_id, features, hit. The important columns are document, sdg, and features. The document number corresponds to the row number of usc_courses dataframe. SDG got changed into 'SDG-01' not 1. Features that are made up of multiple words get split by commas in this column, so I glued them back together. If a course gets mapped to multiple SDGs it will show up multiple rows in the dataframe.

```{r, eval=FALSE}
# duplicate keywords will only count as 1
hits <- detect_any(usc_courses$text, usc_pwg_system, output = "features")
# remove commas in features
hits$cleanfeatures <- gsub(",", "", hits$features)
# get sdg number
hits$sdg_num <- sapply(hits$sdg, function(x) {
  as.numeric(strsplit(x, "-")[[1]][2])
})
```

Then we want the color for the corresponding sdg (and keyword) by merging the dataframe with the original keywords.

```{r, eval=FALSE}
hits_color <- merge(hits, usc_pwg_keywords, 
                    by.x = c("cleanfeatures", "sdg_num"), 
                    by.y = c("keyword", "goal")) %>%
  select(document, sdg_num, cleanfeatures, color)
```

Next, we want to combine the dataframe with our original course info dataframe. In addition we want two columns that summarize a course's keywords and goals.

```{r, eval=FALSE}
master_course_sdg_data <- merge(hits_color, usc_courses, by.x = "document", by.y = "rowID", all.y = TRUE) %>%
  rename(keyword = cleanfeatures, goal = sdg_num) %>%
  select(document, school, courseID, course_title, instructor, section, semester, keyword, goal, color, course_desc, text, department, N.Sections, year, course_level, total_enrolled, all_semesters) %>%
  arrange(courseID) %>%
  group_by(document) %>%
  mutate(all_keywords = paste(unique(keyword), collapse = ","),
         all_goals = paste(sort(unique(goal)), collapse = ","))
```

## Sustainability Related Courses

After mapping, we analyze the goals a course is mapped to and labels it as `SDG-Related`, `Sustainability-Focused`, or `Not Related`.

We have tried multiple methods to determine sustainability classifications, and our current method is as follows:  

* If a course description does not map to any SDGs, it is "Not Related".  
* If a course description maps to at least 1 SDG, we categorize it as "SDG-Related".  
* If a course maps to one or more social/economic SDG (1-5, 8-11, 16, 17) AND one or more environmental SDG (6, 7, 12, 13, 14, 15), then it is labeled "Sustainability Focused".  

Code for achieving these labels are found in the R script `02_using_text2sdg.R`.  

Lastly, we also want a count of the number of occurrences of each keyword in the course description using `str_count`.

## General Education

We were given completely a different set of data for USC's general education requirements. Code for obtaining the GE categories and course titles is found in `03_general_education.R`. In this script, we join the GE data with the course and sustainability data and then go through and ensure that unmapped courses have "Not Related" as the sustainability classification. The resulting dataframe is used in the Shiny App for the general education page. 

## Creating Shiny App

We can assure that anyone using this github repository can replicate the shiny app with little to no coding experience. To learn the basics, refer to [this tutorial](https://rstudio.github.io/shinydashboard/){target='_blank'}.  

If you follow along with the code in the `app.R` file in the "shiny_app" directory, you will understand the structure and functionality of a shiny app.  

One important tip for making various plots in the dashboard is that it is often helpful to create a new R script to generate a dataframe that is easier to work with for the purposes of that plot / function. In the `02_R` directory, the file `sustainability_related_classes.R` containts code to generate `classes_by_sdgs.csv` which is used for one of the barcharts in the dashboard. We found it incredibly helpful to write code to generate plots in another file so you can quickly go through trial and error instead of opening the dashboard every time. Lastly, **Google, ChatGPT and stackOverflow are your coding friends**... Plenty of people out there are struggling with the same things you struggle with in R and Rshiny. 


## Creating a Github Repo

To make a github repository, follow [this tutorial](https://docs.github.com/en/get-started/quickstart/create-a-repo){target='_blank'} and consider downloading the [GitHub Desktop App](https://desktop.github.com/){target='_blank'}. You can also make commits and pushes using the Git button on the top bar of RStudio. 

## Creating a Readme

To create a Readme, familiarize yourself with [Markdown](https://www.markdownguide.org/getting-started){target='_blank'} and [R Markdown](https://rmarkdown.rstudio.com/articles_intro.html){target='_blank'}. In `.Rmd` (R Markdown) files, you can specify the `output` of the document to be a `github_document` and when you "knit" the `.Rmd` file, it will automatically generate a `.md` (markdown) file in the directory which will be displayed on your github page! You can find more information [here](https://rmarkdown.rstudio.com/github_document_format.html). You can also refer to the README.Rmd file to see how Brian Tinsley created this original readme file.



## Updating Data and Shiny App

When the keywords or course data are updated, the way we have been updating the shiny app is by rerunning all of the files in order with the new data. When doing so, we remove the old files from the `Data` folder and the `shiny_app` folder, but we recommend storing them in a backup folder elsewhere in the case that the new run of code doesn't work. 

Which files you will have to rerun is determined by what data you are updating. If the raw course data is updated, you will need to start from the beginning (at `01_cleaning_raw_data/00_raw_usc_data/01_cleaning_scattered_files.R`) and clean and combine all of the school data again. Similarly, if you are adding / fixing keyword mapping issues with context dependencies, you will need to clean the course data again (starting at `01_cleaning_raw_data/03_cleaning_course_descriptions.R`). If you are only updating the keywords list, then you only need to rerun code starting at the mapping of course descriptions (starting at `02_R/01_cleaning_keywords.R`). 

## Questions?

We are very grateful to the developers that responded to our emails and helped us along the way. If you have any questions, comments, or concerns, please reach out to Brian Tinsley: [btinsley@usc.edu](mailto:btinsley@usc.edu) or Julie Hopper [juliehop@usc.edu](mailto:juliehop@usc.edu)

